{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_files(path):\n",
    "    return [join(path, f) for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = join('..', '..', '..', 'Data')\n",
    "\n",
    "pi_path  = join(data_path, 'KGW', '01_PI_Data')\n",
    "gt_path  = join(data_path, 'KGW', '02_GT_Data')\n",
    "dcs_path = join(data_path, 'KGW', '03_DCS_Data')\n",
    "db_path  = join(data_path, 'kgw_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_files  = all_files(pi_path)\n",
    "gt_files  = all_files(gt_path)\n",
    "dcs_files = all_files(dcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing CSV data\n",
    "\n",
    "Please ensure that the following functions are run only once to avoid multiple entries for the same data in the DB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PI data CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(f, xs):\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)    \n",
    "    for x in xs:\n",
    "        groups[f(x)].append(x)\n",
    "    return groups.values()\n",
    "\n",
    "\n",
    "def merge_all(frames, on=\"TIMESTAMP\", how='inner'):\n",
    "    if len(frames) == 1:\n",
    "        return frames[0]\n",
    "    \n",
    "    from functools import reduce\n",
    "    def merge(a, b): \n",
    "        pd.merge(a, b, on=on, how=how)\n",
    "    return reduce(merge, frames)\n",
    "        \n",
    "\n",
    "def import_data(files, datetime_format='%d.%m.%Y %H:%M'):\n",
    "    date_format = lambda x: pd.datetime.strptime(x, datetime_format)\n",
    "    \n",
    "    def parse(p):\n",
    "        return pd.read_csv(p, \n",
    "                           parse_dates=['TIMESTAMP'], \n",
    "                           infer_datetime_format=True, \n",
    "                           date_parser=date_format, \n",
    "                           sep=';', \n",
    "                           engine='c', \n",
    "                           low_memory=False, \n",
    "                           na_values=[\"#NV\", '?0,0', '?-17,0'], \n",
    "                           decimal=',', \n",
    "                           thousands='.', \n",
    "                           memory_map=True)\n",
    "    \n",
    "    def read(files):\n",
    "        return pd.concat([parse(f) for f in files])\n",
    "\n",
    "    def read_basic(files):\n",
    "        return read(files)\n",
    "    \n",
    "    def read_extra(files):\n",
    "        import re\n",
    "\n",
    "        def name_index(file): \n",
    "            return int(re.search(r'#(\\d+)', file).group(1))\n",
    "\n",
    "        groups = group_by(name_index, files)           \n",
    "        frames = [read(fs) for fs in groups]\n",
    "  \n",
    "        return merge_all(frames)\n",
    "    \n",
    "    basic_files = [f for f in files if \"#\" not in f]\n",
    "    extra_files = [f for f in files if \"#\" in f]\n",
    "\n",
    "    if extra_files:\n",
    "        basic_df = read_basic(basic_files)\n",
    "        extra_df = read_extra(extra_files)\n",
    "        final_df = pd.merge(basic_df, extra_df, on='TIMESTAMP') \n",
    "        print(f'merging with extra data\\nnumber of rows\\nbasic: {len(basic_df)}\\nextra: {len(extra_df)}\\nfinal: {len(final_df)}\\n-------------\\ndiff:  {len(basic_df) - len(final_df)}')\n",
    "        return final_df\n",
    "    else:\n",
    "        return read_basic(basic_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging with extra data\n",
      "number of rows\n",
      "basic: 403140\n",
      "extra: 403140\n",
      "final: 403140\n",
      "-------------\n",
      "diff:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/Develop/envs/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 14s, sys: 29.7 s, total: 6min 43s\n",
      "Wall time: 52.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pi_df = import_data(pi_files)\n",
    "pi_df = pi_df.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GT data CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/Develop/envs/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.15 s, sys: 1.01 s, total: 8.16 s\n",
      "Wall time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gt_df = import_data(gt_files)\n",
    "gt_df = gt_df.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import DCS data CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab/Develop/envs/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 717 ms, total: 15.3 s\n",
      "Wall time: 5.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dcs_df = import_data(dcs_files, datetime_format='%d.%m.%Y %H:%M:%S')\n",
    "dcs_df = dcs_df.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dcs    = pd.merge(gt_df,  dcs_df, on='TIMESTAMP', how='outer')\n",
    "gt_dcs_pi = pd.merge(gt_dcs, pi_df,  on='TIMESTAMP', how='outer')\n",
    "final_df = gt_dcs_pi.sort_values('TIMESTAMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast 'on / of' columns to 1 = on and 0 = of\n",
    "def get(s):\n",
    "    value_dict = { 'aus': 0, 'ein': 1, 'nan': np.nan, 'bad input': np.nan }   \n",
    "    k = str(s).strip().lower()\n",
    "    if k in value_dict:\n",
    "        return value_dict[k]\n",
    "    else:\n",
    "        print(f'unkown value: --{s}--')\n",
    "        return np.nan\n",
    "\n",
    "for c in ['120 A0 LAC60 AP001XB01', '120 A0 LAC65 AP001XB01', '120 A0 LAC70 AP001XB01', '120 A0 LAC75 AP001XB01', '120 A0 LAC80 AP001XB01']:\n",
    "    final_df[c] = final_df[c].apply(get)\n",
    "    \n",
    "    \n",
    "# ensure all data columns are 32bit floats instead of 64bit\n",
    "for c in final_df.columns[1:]:\n",
    "    final_df[c] = final_df[c].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 416315 entries, 18063 to 40252\n",
      "Columns: 746 entries, TIMESTAMP to 053 A6 MAY50 GH010ZXQ51\n",
      "dtypes: datetime64[ns](1), float32(745)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data Frames in HDF5 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = join('..', '..', '..', 'Data', 'kgw_data.h5')\n",
    "\n",
    "with pd.HDFStore(db_path) as DB:\n",
    "    DB.put(key='pi_data',  value=pi_df)\n",
    "    DB.put(key='gt_data',  value=gt_df)\n",
    "    DB.put(key='dcs_data', value=dcs_df)\n",
    "    DB.put(key='all_data', value=final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Frames from HDF5 DB\n",
    "\n",
    "The following snippet should used in the notebook used for analysing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87 ms, sys: 1.11 s, total: 1.19 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "\n",
    "db_path = join('..', '..', '..', 'Data', 'kgw_data.h5')\n",
    "\n",
    "pi_df  = pd.read_hdf(db_path, key='pi_data',  format='t')\n",
    "gt_df  = pd.read_hdf(db_path, key='gt_data',  format='t')\n",
    "dcs_df = pd.read_hdf(db_path, key='dcs_data', format='t')\n",
    "all_df = pd.read_hdf(db_path, key='all_data', format='t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 416315 entries, 18063 to 40252\n",
      "Columns: 746 entries, TIMESTAMP to 053 A6 MAY50 GH010ZXQ51\n",
      "dtypes: datetime64[ns](1), float32(745)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TIMESTAMP                       datetime64[ns]\n",
       "AN_AGb_Acceleration                    float32\n",
       "AN_AGB_Accel_Band0                     float32\n",
       "AN_AGB_Accel_Band1                     float32\n",
       "AN_AGB_Accel_Band2                     float32\n",
       "AN_AGB_Accel_Band3                     float32\n",
       "AN_AGB_Hours                           float32\n",
       "AN_AGB_Start_Count                     float32\n",
       "AN_Air_Supply_Pressure                 float32\n",
       "AN_Ambient_Air_Humidity                float32\n",
       "AN_Ambient_Air_Temperature             float32\n",
       "AN_BDEW_Bus_Volt                       float32\n",
       "AN_BDEW_Bus_Volt_Pcnt                  float32\n",
       "AN_BDEW_Delta_P                        float32\n",
       "AN_BDEW_High_Freq_P_Limit              float32\n",
       "AN_BDEW_Nom_Util_Volt                  float32\n",
       "AN_BDEW_Pwr_Lim                        float32\n",
       "AN_BDEW_P_Ist                          float32\n",
       "AN_BDEW_Rated_Power                    float32\n",
       "AN_BDEW_Signals_P_Limit                float32\n",
       "AN_BDEW_Signals_P_Limit_Pcnt           float32\n",
       "AN_BDEW_Sync_Freq_High                 float32\n",
       "AN_BDEW_Sync_Freq_Low                  float32\n",
       "AN_BDEW_Sync_Volt_High                 float32\n",
       "AN_BDEW_Sync_Volt_Low                  float32\n",
       "AN_Bleed_Valve_Command                 float32\n",
       "AN_Bleed_Valve_Position                float32\n",
       "AN_Bus_Avg_LL_V                        float32\n",
       "AN_Bus_A_PhA_V                         float32\n",
       "AN_Bus_B_PhA_V                         float32\n",
       "                                     ...      \n",
       "120 A6 LBA01 FF001XQ01                 float32\n",
       "120 A6 LBA01 FF001ZXQ43                float32\n",
       "120 A7 HAD01 CL001XQ01                 float32\n",
       "120 A7 HAD01 CP001XQ01                 float32\n",
       "120 A7 HAD01 CQ001XQ01                 float32\n",
       "120 A7 HAH01 CT001XQ01                 float32\n",
       "120 A7 HHG01 FF001AXQ01                float32\n",
       "120 A7 LAB01 CF001XQ01                 float32\n",
       "120 A7 LBA01 FF001XQ01                 float32\n",
       "120 A7 LBA01 FF001ZXQ43                float32\n",
       "120 A8 HAD01 CL001XQ01                 float32\n",
       "120 A8 HAD01 CP001XQ01                 float32\n",
       "120 A8 HAD01 CQ001XQ01                 float32\n",
       "120 A8 HAH01 CT001XQ01                 float32\n",
       "120 A8 HAY01 BZ001XQ51                 float32\n",
       "120 A8 LAB01 CF001XQ01                 float32\n",
       "120 A8 LBA01 FF001XQ01                 float32\n",
       "120 A8 LBA01 FF001ZXQ43                float32\n",
       "120 A5 HAD10 CP001XQ01                 float32\n",
       "120 A5 HNA20 CT041XQ01                 float32\n",
       "120 A5 HNA20 CF001XQ01                 float32\n",
       "120 A5 HNA20 CQ001XQ11                 float32\n",
       "120 A5 HNA20 CQ002XQ11                 float32\n",
       "120 A5 HNA20 CQ003XQ11                 float32\n",
       "097 A0 EKA01 CF001XQ01                 float32\n",
       "120 A1 EKA10 FF001BXQ01                float32\n",
       "120 A1 EKA20 FF001BXQ01                float32\n",
       "120 A7 HAY01 BZ001XQ51                 float32\n",
       "053 A0 EKA01 CF001XQ01                 float32\n",
       "053 A6 MAY50 GH010ZXQ51                float32\n",
       "Length: 746, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
